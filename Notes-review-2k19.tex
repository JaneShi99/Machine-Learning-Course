

\documentclass[12pt]{article} 


%%%%%%% a few packages
\usepackage{fullpage}
\usepackage{todonotes}
\usepackage{color}
\usepackage{hyperref} % for the URL
\usepackage{pst-tree} % for the trees
\usepackage{verbatim} 
\usepackage{ifthen} 
\usepackage{amsmath}
\usepackage{listings}  
\usepackage{amssymb}
\usepackage{array}
\usepackage{multicol}
\usepackage{tikz}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
	language=Python,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}


%%%%%% for pst-tree, define a node that ALWAYS has the same width (here "99")
\newlength{\nodeLength}
\newcommand{\Node}{A}
\newcommand{\setnode}[1]{ \settowidth{\nodeLength}{#1}
  \renewcommand{\Node}[1]{ \Tcircle{\makebox[\nodeLength]{##1}} }
}
\setnode{99}

%%%%%%%%%%%%%%%%%%%%% algo.sty copied over %%%%%%%%%%%%%%%%%%%%%
\newcounter{algorithmeligne}
\newcommand{\instr}[1]{%\underline
                        {\bf #1}}
\newcommand{\nomproc}[1]{{\rm\bf #1}}
\newenvironment{algorithme} { \setcounter{algorithmeligne}{0}
        \newcommand{\lign}{\stepcounter{algorithmeligne}
                \>{\footnotesize \arabic{algorithmeligne}.}\> }
        \begin{center}
        \begin{tabular}{|c|}
        \hline
        \begin{minipage}{1cm} \small \, \ \ \\[-11mm] \begin{tabbing}
\=\hskip1cm\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\\\kill 
        }
        {
        \end{tabbing}
        \ \ \\[-8mm]
        \end{minipage}
        \\\hline
        \end{tabular}
        \end{center}
        }


\setlength{\parskip}{0.25cm plus 4mm minus 3mm}

\renewcommand\labelitemi{-}

\begin{document}

\begin{center}
{\Large \bf Coursera- Machine Learning}\\
\vspace{3mm}
{\Large \bf May 2019}\\
\vspace{3mm}
{\Large \bf Taught by Prof. Andrew Ng}\\
\vspace{3mm}
\textbf{Janeshi99}\\
\end{center}


\section*{Summary}
Supervised learning
\begin{itemize}
	\item linear regression, logistic regression, neural network, SVMs
\end{itemize}
Unsupervised learning
\begin{itemize}
	\item k-means, PCA, Anomaly detection
\end{itemize}
Special applications/special topics
\begin{itemize}
	\item Recommender systems, large scale machine learning
	
\end{itemize}
Advice for building a machine learning system
\begin{itemize}
	\item bias/variance, regularization, deciding what to work next, evaluation of a learning algorithm, learning curves, error analysis, ceiling analysis
\end{itemize}


\section*{Week 1}

\underline{Intro}\\
Definition of ML\\
\begin{itemize}
	\item A program learns from experience (E) w.r.t task(T) and performance measure (P) if its performance on T improves with more E.\\
	\item With supervised learning, we know what our answers are as a relation of input and output. But with unsupervised learning, we have little idea about the result.\\
\end{itemize}

\underline{Cost function}\\
\begin{itemize}
	\item \[h_{\theta}(x) = \theta_0 + \theta_1x\]
	 our goal is to minimize the cost function, which is calculated as square error
	 \[\min_{\theta_0,\theta_1} J(\theta_0,\theta_1)\]
	\item where the error function is defined as
	 \[J(\theta_0,\theta_1) = \frac{1}{2m} \sum_{}^{}(h_\theta(x^{(i)} - y ^{(i)})^2)\]
\end{itemize}

\underline{Linear regression}
\begin{center}
	\begin{itemize}
		\item 	Repeat until converge\{
		$  \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1) \text{ for } j = 0,1 $ \}	
		\item  Note that the update is \underline{simultaneous} :
		\item 
		\[ \text{temp}_0 := \theta_0 - \alpha\frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1) \] 
		\[ \text{temp}_1 := \theta_1 - \alpha\frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1) \]
		\[\theta_0 := \text{temp}_0 \]
		\[\theta_1 := \text{temp}_1\]
		\item if we compute the derivative we get
		Repeat until converge\{
		
		\[ \theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i) \] 
		\[ \theta_1 := \theta_1 - \alpha\frac{1}{m}\sum_{i=1}^{m}((h_{\theta}(x_i)-y_i) x_i)\] 
		
		\}	
		
		\item $\alpha$ is the learning rate. 
		\item we use linear regression algorithm to updates the parameters until we arrive at the minimal cost.
	\end{itemize}

\end{center}

\section*{Week 2}

\underline{Multi-feature linear regression}\\
\begin{itemize}
\item Hypothesis
\[h_\theta (x) = \theta_0+ \theta_1x_1 +\ldots \theta_nx_n\]
\item convenience $\forall x$, $x_0=1$, so that $h_\theta= \sum_{i=0}^{n}\theta_ix_i$
\item 
$$ x= \begin{bmatrix}
	x_0 \\
	x_1 \\
	\vdots \\
	x_n
\end{bmatrix}
 \in \mathbb{R} ^{n+1}
\text{ and that } \theta = \begin{bmatrix}
	\theta_0 \\
	\theta_1 \\
	\vdots \\
	\theta_n
\end{bmatrix}
$$
\item Hypothesis can be represented as
\[h_\theta(x)=\theta^T x \text{ or } <\theta,x>\]
\item The parameter we're estimating here is $\theta$
\item Cost function 
\[J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}((h_\theta)x^{(i)})-y^{(i)})^2\]
\item Gradient descent
\[repeat \{ \theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}  \text{ ,simultaneously update }\theta_0\ldots \theta_j\}\]
\item When working with gradient descent in practice, we should... consider
\item Feature scaling:\\
Make sure features are in a similar scale, so that each values are roughly between $[-3,3]$
\item Mean normalization:
Replace all $x_i$ (except for $x_0$ )with $x_i-\mu_i$ so that the mean is roughly $0$.\\
\[x_i\leftarrow \frac{x_i-u_i}{s_i}\]
\item Note that $J$ should always decresase w.r.t to the number of iteration. If it ever increases, that means our $\alpha$, the step param, is too large. We would want to decrease $\alpha$.\\
\item Pick $\epsilon$ for the convergence threshold value.
\item Tip: in order to choose $\alpha$, try a range of values. Example: choosing based on a logarithmic scale: \[0.001,0.003,0.01,0.03,\ldots\]\\

\end{itemize}

\underline{feature \& polynomial regression}
\begin{itemize}
	\item We can combine multiple features into one and change the behaviour of the hypothesis.
	\item For example we can combine $x_1. x_2$ into a polynomial term by defining that $x_3=x_1*x_2$.
	\item polynomial regression, instead of linear, we make it quadratic or cubic to tune the hypothesis
	\[h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2^2 + \theta_3\sqrt{x}\]
	Keep in mind that feature scaling is still very important.\\
	
\end{itemize}

\underline{Normal equation: computing parameters analytically}
\begin{itemize}
\item Define $X$ as the design matrix. That is 
$$ \text{if } x^{(i)}= \begin{bmatrix}
x_0^{(i)} \\
x_1^{(i)} \\
\vdots \\
x_n^{(i)} \end{bmatrix}
\text{ then we have }
X =
\left[
\begin{array}{ccc}
-& (x^{(1)})^T & - \\
- & (x^{(2)})^T & - \\
& \vdots    &          \\
- & (x^{(n)})^T & -
\end{array}
\right]
$$
\item Optimum $\theta$ given by $\theta = (X^TX)^{-1}X^Ty$
\item With normal equation, you don't need feature scaling. 
	\begin{multicols}{2}
	\textit{Gradient descent}:\\
	$\alpha$ needs to be chosen\\
	needs many iterations\\
	works well even when $n$ is large\\
	
		\columnbreak
		
		\textit{Normal equation}:\\
		no need to choose $\alpha$\\
		no iterations needed\\
		computing $(X^TX)^{-1}$ takes $O(n^3)$\\
		performs slow with large $n (n\geq 10,000)$
	\end{multicols}
\item Note: what if $X^TX$ is non-invertible? Then we use the $pinv$ to generate the pseudo-inverse.
\end{itemize} 

\underline{vectorization} helps to compute vectors faster.

\section*{Week 3}

\underline{Binary classification problems}
\begin{itemize}
	\item each element $y$ belongs to negative class ($0$) or positive class ($1$).
\end{itemize}

\underline{Logistic regression}
	\begin{itemize}
		\item We want $0\leq h_\theta(x) \leq 1$\\
		$h_\theta(x) = g(\theta^Tx)$ where $g$ is the sigmoid function\\
		$g(z) = \frac{1}{1+e^{-z}}$
	\end{itemize}

\underline{decision boundary}
\begin{itemize}

\item The decision boundary is the line that separates area where $y=0$ or $y=1$.
\item Note that 
\[h_\theta(x)\geq0.5\iff \theta^TX\geq 0 \to y=1\]
\[h_\theta(x)<0.5\iff \theta^TX< 0 \to y=0\]
	\item we can also work with non-linear decision boundaries
\end{itemize}

\underline{Logistic regression model}
\begin{itemize}
	
	\item The training set will be $\{((x^{(1)},y^{(1)}), \ldots (x^{(m)},y^{(m)})))\}$\\
	There are $m$ examples, and for $\forall x, x\in \mathbb{R}^{n+1}$, $x_0=1$, $y\in\{0,1\}$\\
	$h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}$
	\item We realize that lin.reg. will not give you a convex function but we \underline{want} a convex function. This brings us to construct a good cost function.
	\item 
	 \[
	cost(h_\theta(x),y)=\left\{
	\begin{array}{ll}
	-\log(h_\theta(x)) &\text{ if }y=1\\
	-\log(1-h_\theta(x))& \text{ if }y=0
	\end{array}
	\right.
	\]
	\item For example if $y=1$ then if $x=1$ we have cost=$0$. And as hypothesis approach $0$, cost approaches $\infty$ so we're penalized. Similar with the other situation.
	\item This gives us a convex and local optimum free function.
	\item The \underline{uncompressed cost function } is:
	\[\text{cost } (h_\theta(x),y) = -y\log(h_\theta(x))-(1-y)log(1-h_\theta(x))\]
	\item The total cost unction $J$:
	\[j(\theta) = \frac{1}{m}\sum_{i=1}^{m} \text{cost } (h_\theta(x),y) \]
	\item The gradient descent algorithm is essetially the same but referring to a different hypothsis which is $h_\theta(x)$ that now refers to the sigmoid function\\
	\end{itemize}

\underline{The vectorized implementation}

\begin{itemize}
	\item $h=g(X\theta)$, which computes the quantity $h_\theta(x^{(i)})$
	\item
	$J(\theta) = \frac{1}{m}(-y^T\log(h)-(1-y)^T\log(1-h))$
\end{itemize}

\underline{Gradient descent}
\begin{itemize}
	\item Idea is to re-arrange the vectors until it's easier to type into matlab.
	\item Reminder that the matrix $X$ looks like this: 
	$$X =
	\left[
	\begin{array}{ccc}
	-& (x^{(1)})^T & - \\
	- & (x^{(2)})^T & - \\
	& \vdots    &          \\
	- & (x^{(m)})^T & -
	\end{array}
	\right]
	$$
	\item X is a $m\times n$ matrix (ignoring the extra leftmost column of $1$s). $\theta$ is a $n\times1$ vector, which makes $X^T\theta$ a $m\times1$ vector which yields the answer.
	\item \[ \theta:= \theta -\frac{\alpha}{m} \sum_{i=1}^{m} [(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j]\] note that the $x$ is a column vector.
	\item the vectorized version is: \[\theta:=\theta - \frac{\alpha}{m}X^T(g(X\theta)-\bar{y})\]
	\item Advanced optimization: Given the cost function $J(\theta)$ and gradient $\frac{\partial}{\partial\theta_j}J(\theta)$ we can compute $\min_\theta J(\theta)$. 
	\item These following alorithms are more complex but compute $\theta$ at a faster rate, and there is no need to compute $\alpha$. 
	\item Conjugate gradient, BFGS, L-BGFGS
	\item Use the function "fminunc()"
	
\end{itemize}

\underline{Logistic optmization for multiple classes}
\begin{itemize}
	\item Multiple classification is the situation when $y=\{0,\ldots n\}$. i.e. we have different classes of outcomes. Our strategy is to assign one class as 'positive' and the rest of classes as 'the rest'.
	\item $y\in\{0,1,2,\ldots,n\}$
	\item $h_\theta^{(0)}(x) = P(y=0\mid x;\theta)$
		\item $h_\theta^{(1)}(x) = P(y=1\mid x;\theta)$
		\item $\vdots$
			\item $h_\theta^{(n)}(x) = P(y=n\mid x;\theta)$
	\item The prediction is $\max_i h_\theta^{(i)}(x)$
\end{itemize}

\underline{The problem of over-fitting}
\begin{itemize}
	\item Underfitting: the hypothesis function fits too poorly onto the trend of the data, the function is either too simple or accounting too little features.
	\item Overfitting: the hypothesis function is not generalized enough. It fits well with given data but presents unnecessary corners/ angles.
	\item To resolve overfitting, we can either 1) reduce the number of features, i.e. have an algorithm that ditches unimportant features. Or 2) apply regularization to reduce magnitude of some features.
\end{itemize}

\underline{Cost function} with regularization
\begin{itemize}
	\item \[\min_\theta \frac{1}{2m}\sum_{i=1}{m} (h_\theta (x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}{n}\theta_j^2\]
	\item We want a big $\lambda$ so that bumps up and forces $\theta_j$ to be small as we penalize bit $\theta_j$.
	
\end{itemize}

\underline{Gradient descent}

\begin{itemize}
	\item \begin{align*}
\text{repeat } \{ &
\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \text{ and}\\
&\theta_j := \theta_j - \alpha\big[\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j\big] \\ 
&\text{ for } j = \{1,\ldots n\}	\end{align*}
	\item or we can equivalently write this in one equation:
	\[\theta_j := \theta_j(1-\alpha\frac{\lambda}{m} - \alpha\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} ) \]
\end{itemize}

\underline{Normal Equation}
\begin{itemize}
	\item \[\theta = (X^TX+\lambda L)^{-1} X^Ty\]
	\item where $L$ is the same as $I \in M_n^{(n+1)\times (n+1) }$ 
	with the first $1$ replaced with $0$. Note that if $m<n$ then $X^TX$ is non-invertible but adding $L$ makes it invertible. Hence regulartization also solves non-invertability.	
\end{itemize}

\underline{Regularized logistic equation} \\
(note that advanced optimization method me mentioned earlier also works here)

\begin{itemize}
	\item Regularized cost function for logistic regression is:
	\[J(\theta) = -\frac{1}{m}\sum_{i=1}^{m} \big[y^{(i)}\log(h_\theta (x^{(i)} ))+ (1-y^{(i)}) \log (1-h_\theta(x^{(i)}))\big] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2\] 
	\item the last term is newly added fort he regularization
	\item Gradient descent is exactly the same	\item \begin{align*}
	\text{repeat } \{ &
	\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \text{ and}\\
	&\theta_j := \theta_j - \alpha\big[\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j\big] \\ 
	&\text{ for } j = \{1,\ldots n\}	\end{align*}
\end{itemize}

\underline{Advanced functions} (regularization)
\begin{itemize}
	\item $Jval$ does not change while the gradients do change.
	\item Gradient $1$(index $0$):
	\[\frac{1}{m} \sum_{i=1}^{m}(h_\theta(X^{(i)} )- y^{(i)})x_0^{(i)} \]
	\item Gradient $2$(index $1,2,\ldots n$)
	\[\big(   \frac{1}{m} \sum_{i=1}^{m}(h_\theta(X^{(i)} )- y^{(i)})x_0^{(i)}   \big) +\frac{\lambda}{m}\theta_j\] note that the last term is a newly added item.
\end{itemize}

\section*{Week 4}

\underline{Neural Networks model representation}

\begin{itemize}
	\item We use matrices and vectors to model neurons and layers. [I'm too lazy to use tikz to draw it]
	\item Neural network is consisted of multiple layers. There is an input-layer, lots of hidden layer in the middle and the output layer wich is one node. 
	\item $a_i^{(j)} = $ "activation" of unit $i$ in layer $j$.
	\item $\Theta^{(j)}$ = matrix of weights controlling function mapping from layer $j$ to layer $j+1$.
	\item \underline{Vec representation}
	 \item 
	 $$  \begin{bmatrix}
	 x_0 \\
	 x_1 \\
	 x_2 \\
	 x_3 \end{bmatrix}
	 \to
	 \left[
	 \begin{array}{ccc}
	 a_1^{(2)}\\
	 a_2^{(2)} \\
	 a_3^{(2)} 
	 \end{array}
	 \right]
	 \to h_\Theta(x)
	 $$
	 \item Layer 1 to layer 2:
	\begin{align*}
	a_1^{(2)} = & g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3 )\\	a_2^{(2)} = & g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3 )\\
	a_3^{(2)} = & g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3 )\\
	\end{align*}
	\item Layer 2 to layer 3:
	\[
	h_\Theta(x)=a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)}  + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)}+ \Theta_{13}^{(2)}a_3^{(2)}  )
	\]
	\item \underline{What is the dimension of $\Theta$?}
	\item Note that each layer has its own matrix of weights. If network has $S_j$ layers in level $j$ and $S_{j+1}$ layers in level $j+1$ then $\Theta^{(j)}$ has dimensions $S_{j+1}\times (S_j+1)$ where the extra $1$ comes from the bias node.
	\item the intuition is that NN allows nodes in its hidden layer to 'learn' its own features.
	
\end{itemize}

\underline{Vectorization of Computation}

\begin{itemize}
	\item $a_1^{(2)} = g(z_1^{(2)})$
	\item $a_2^{(2)} = g(z_2^{(2)})$
	\item $a_3^{(2)} = g(z_3^{(2)})$
	\item For layer $j$ , node $k$, $z$ is $z_k^{(j)} = \Theta_{k0}^{(j-1)}x_0 + \Theta_{k1}^{(j-1)}x_1+\ldots + \Theta_{kn}^{(j-1)}x_n$
	\item 
	$$  \begin{bmatrix}
	x_0 \\
	x_1 \\
	\vdots\\
	x_n \end{bmatrix}
	z^{(j)}
	\left[
	\begin{array}{ccc} 
	z_1^{(j)}\\
	z_2^{(j)} \\
	\vdots\\
	z_n^{(j)} 
	\end{array}
	\right]
	\to h_\Theta(x)
	$$
	\item note that \[z^{(j)} = \Theta^{(j-1)}a^{(j-1)}\] since dimentions of $\Theta$ is $S-j\times n+1$ and that dimensions of $a$ is $n+1\times 1$.
	\item We add the biad unit to layer $j$ after computing $a^{(j)}$ i.e. $a_0^{(j)} = 1$.
	\item In order to compute the fial hpothesis, we compute the final $z$ vector, where the last matrix of $\Theta$ only has $1$ row, which mulipled by a volumn will result in a real number. \[h_\Theta(x) = a^{(j+1)}=g(z^{(j+1)})\]
	\item Multi-class classification works similarly with the one-vs all method. We set $h_\Theta(x)\in\mathbb{R}^4$ if there are $4$ classes. \\
	Then 
\end{itemize}
\end{document}