

\documentclass[12pt]{article} 


%%%%%%% a few packages
\usepackage{fullpage}
\usepackage{todonotes}
\usepackage{color}
\usepackage{hyperref} % for the URL
\usepackage{pst-tree} % for the trees
\usepackage{verbatim} 
\usepackage{ifthen} 
\usepackage{amsmath}
\usepackage{listings}  
\usepackage{amssymb}
\usepackage{array}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{tikz}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
	language=Python,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}


%%%%%% for pst-tree, define a node that ALWAYS has the same width (here "99")
\newlength{\nodeLength}
\newcommand{\Node}{A}
\newcommand{\setnode}[1]{ \settowidth{\nodeLength}{#1}
  \renewcommand{\Node}[1]{ \Tcircle{\makebox[\nodeLength]{##1}} }
}
\setnode{99}

%%%%%%%%%%%%%%%%%%%%% algo.sty copied over %%%%%%%%%%%%%%%%%%%%%
\newcounter{algorithmeligne}
\newcommand{\instr}[1]{%\underline
                        {\bf #1}}
\newcommand{\nomproc}[1]{{\rm\bf #1}}
\newenvironment{algorithme} { \setcounter{algorithmeligne}{0}
        \newcommand{\lign}{\stepcounter{algorithmeligne}
                \>{\footnotesize \arabic{algorithmeligne}.}\> }
        \begin{center}
        \begin{tabular}{|c|}
        \hline
        \begin{minipage}{1cm} \small \, \ \ \\[-11mm] \begin{tabbing}
\=\hskip1cm\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\=\qquad\\\kill 
        }
        {
        \end{tabbing}
        \ \ \\[-8mm]
        \end{minipage}
        \\\hline
        \end{tabular}
        \end{center}
        }


\setlength{\parskip}{0.25cm plus 4mm minus 3mm}

\renewcommand\labelitemi{-}

\begin{document}

\begin{center}
{\Large \bf Coursera- Machine Learning}\\
\vspace{3mm}
{\Large \bf May 2019}\\
\vspace{3mm}
{\Large \bf Taught by Prof. Andrew Ng}\\
\vspace{3mm}
\textbf{Janeshi99}\\
\end{center}


\section*{Summary}
Supervised learning
\begin{itemize}
	\item linear regression, logistic regression, neural network, SVMs
\end{itemize}
Unsupervised learning
\begin{itemize}
	\item k-means, PCA, Anomaly detection
\end{itemize}
Special applications/special topics
\begin{itemize}
	\item Recommender systems, large scale machine learning
	
\end{itemize}
Advice for building a machine learning system
\begin{itemize}
	\item bias/variance, regularization, deciding what to work next, evaluation of a learning algorithm, learning curves, error analysis, ceiling analysis
\end{itemize}

\newpage
\section*{Week 1}

\underline{Intro}\\
Definition of ML\\
\begin{itemize}
	\item A program learns from experience (E) w.r.t task(T) and performance measure (P) if its performance on T improves with more E.\\
	\item With supervised learning, we know what our answers are as a relation of input and output. But with unsupervised learning, we have little idea about the result.\\
\end{itemize}

\underline{Cost function}\\
\begin{itemize}
	\item \[h_{\theta}(x) = \theta_0 + \theta_1x\]
	 our goal is to minimize the cost function, which is calculated as square error
	 \[\min_{\theta_0,\theta_1} J(\theta_0,\theta_1)\]
	\item where the error function is defined as
	 \[J(\theta_0,\theta_1) = \frac{1}{2m} \sum_{}^{}(h_\theta(x^{(i)} - y ^{(i)})^2)\]
\end{itemize}

\underline{Linear regression}
\begin{center}
	\begin{itemize}
		\item 	Repeat until converge\{
		$  \theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1) \text{ for } j = 0,1 $ \}	
		\item  Note that the update is \underline{simultaneous} :
		\item 
		\[ \text{temp}_0 := \theta_0 - \alpha\frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1) \] 
		\[ \text{temp}_1 := \theta_1 - \alpha\frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1) \]
		\[\theta_0 := \text{temp}_0 \]
		\[\theta_1 := \text{temp}_1\]
		\item if we compute the derivative we get
		Repeat until converge\{
		
		\[ \theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i) \] 
		\[ \theta_1 := \theta_1 - \alpha\frac{1}{m}\sum_{i=1}^{m}((h_{\theta}(x_i)-y_i) x_i)\] 
		
		\}	
		
		\item $\alpha$ is the learning rate. 
		\item we use linear regression algorithm to updates the parameters until we arrive at the minimal cost.
	\end{itemize}

\end{center}
\newpage
\section*{Week 2}

\underline{Multi-feature linear regression}\\
\begin{itemize}
\item Hypothesis
\[h_\theta (x) = \theta_0+ \theta_1x_1 +\ldots \theta_nx_n\]
\item convenience $\forall x$, $x_0=1$, so that $h_\theta= \sum_{i=0}^{n}\theta_ix_i$
\item 
$$ x= \begin{bmatrix}
	x_0 \\
	x_1 \\
	\vdots \\
	x_n
\end{bmatrix}
 \in \mathbb{R} ^{n+1}
\text{ and that } \theta = \begin{bmatrix}
	\theta_0 \\
	\theta_1 \\
	\vdots \\
	\theta_n
\end{bmatrix}
$$
\item Hypothesis can be represented as
\[h_\theta(x)=\theta^T x \text{ or } <\theta,x>\]
\item The parameter we're estimating here is $\theta$
\item Cost function 
\[J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}((h_\theta)x^{(i)})-y^{(i)})^2\]
\item Gradient descent
\[repeat \{ \theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}  \text{ ,simultaneously update }\theta_0\ldots \theta_j\}\]
\item When working with gradient descent in practice, we should... consider
\item Feature scaling:\\
Make sure features are in a similar scale, so that each values are roughly between $[-3,3]$
\item Mean normalization:
Replace all $x_i$ (except for $x_0$ )with $x_i-\mu_i$ so that the mean is roughly $0$.\\
\[x_i\leftarrow \frac{x_i-u_i}{s_i}\]
\item Note that $J$ should always decresase w.r.t to the number of iteration. If it ever increases, that means our $\alpha$, the step param, is too large. We would want to decrease $\alpha$.\\
\item Pick $\epsilon$ for the convergence threshold value.
\item Tip: in order to choose $\alpha$, try a range of values. Example: choosing based on a logarithmic scale: \[0.001,0.003,0.01,0.03,\ldots\]\\

\end{itemize}

\underline{feature \& polynomial regression}
\begin{itemize}
	\item We can combine multiple features into one and change the behaviour of the hypothesis.
	\item For example we can combine $x_1. x_2$ into a polynomial term by defining that $x_3=x_1*x_2$.
	\item polynomial regression, instead of linear, we make it quadratic or cubic to tune the hypothesis
	\[h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2^2 + \theta_3\sqrt{x}\]
	Keep in mind that feature scaling is still very important.\\
	
\end{itemize}

\underline{Normal equation: computing parameters analytically}
\begin{itemize}
\item Define $X$ as the design matrix. That is 
$$ \text{if } x^{(i)}= \begin{bmatrix}
x_0^{(i)} \\
x_1^{(i)} \\
\vdots \\
x_n^{(i)} \end{bmatrix}
\text{ then we have }
X =
\left[
\begin{array}{ccc}
-& (x^{(1)})^T & - \\
- & (x^{(2)})^T & - \\
& \vdots    &          \\
- & (x^{(n)})^T & -
\end{array}
\right]
$$
\item Optimum $\theta$ given by $\theta = (X^TX)^{-1}X^Ty$
\item With normal equation, you don't need feature scaling. 
	\begin{multicols}{2}
	\textit{Gradient descent}:\\
	$\alpha$ needs to be chosen\\
	needs many iterations\\
	works well even when $n$ is large\\
	
		\columnbreak
		
		\textit{Normal equation}:\\
		no need to choose $\alpha$\\
		no iterations needed\\
		computing $(X^TX)^{-1}$ takes $O(n^3)$\\
		performs slow with large $n (n\geq 10,000)$
	\end{multicols}
\item Note: what if $X^TX$ is non-invertible? Then we use the $pinv$ to generate the pseudo-inverse.
\end{itemize} 

\underline{vectorization} helps to compute vectors faster.
\newpage
\section*{Week 3}

\underline{Binary classification problems}
\begin{itemize}
	\item each element $y$ belongs to negative class ($0$) or positive class ($1$).
\end{itemize}

\underline{Logistic regression}
	\begin{itemize}
		\item We want $0\leq h_\theta(x) \leq 1$\\
		$h_\theta(x) = g(\theta^Tx)$ where $g$ is the sigmoid function\\
		$g(z) = \frac{1}{1+e^{-z}}$
	\end{itemize}

\underline{decision boundary}
\begin{itemize}

\item The decision boundary is the line that separates area where $y=0$ or $y=1$.
\item Note that 
\[h_\theta(x)\geq0.5\iff \theta^TX\geq 0 \to y=1\]
\[h_\theta(x)<0.5\iff \theta^TX< 0 \to y=0\]
	\item we can also work with non-linear decision boundaries
\end{itemize}

\underline{Logistic regression model}
\begin{itemize}
	
	\item The training set will be $\{((x^{(1)},y^{(1)}), \ldots (x^{(m)},y^{(m)})))\}$\\
	There are $m$ examples, and for $\forall x, x\in \mathbb{R}^{n+1}$, $x_0=1$, $y\in\{0,1\}$\\
	$h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}$
	\item We realize that lin.reg. will not give you a convex function but we \underline{want} a convex function. This brings us to construct a good cost function.
	\item 
	 \[
	cost(h_\theta(x),y)=\left\{
	\begin{array}{ll}
	-\log(h_\theta(x)) &\text{ if }y=1\\
	-\log(1-h_\theta(x))& \text{ if }y=0
	\end{array}
	\right.
	\]
	\item For example if $y=1$ then if $x=1$ we have cost=$0$. And as hypothesis approach $0$, cost approaches $\infty$ so we're penalized. Similar with the other situation.
	\item This gives us a convex and local optimum free function.
	\item The \underline{uncompressed cost function } is:
	\[\text{cost } (h_\theta(x),y) = -y\log(h_\theta(x))-(1-y)log(1-h_\theta(x))\]
	\item The total cost unction $J$:
	\[j(\theta) = \frac{1}{m}\sum_{i=1}^{m} \text{cost } (h_\theta(x),y) \]
	\item The gradient descent algorithm is essetially the same but referring to a different hypothsis which is $h_\theta(x)$ that now refers to the sigmoid function\\
	\end{itemize}

\underline{The vectorized implementation}

\begin{itemize}
	\item $h=g(X\theta)$, which computes the quantity $h_\theta(x^{(i)})$
	\item
	$J(\theta) = \frac{1}{m}(-y^T\log(h)-(1-y)^T\log(1-h))$
\end{itemize}

\underline{Gradient descent}
\begin{itemize}
	\item Idea is to re-arrange the vectors until it's easier to type into matlab.
	\item Reminder that the matrix $X$ looks like this: 
	$$X =
	\left[
	\begin{array}{ccc}
	-& (x^{(1)})^T & - \\
	- & (x^{(2)})^T & - \\
	& \vdots    &          \\
	- & (x^{(m)})^T & -
	\end{array}
	\right]
	$$
	\item X is a $m\times n$ matrix (ignoring the extra leftmost column of $1$s). $\theta$ is a $n\times1$ vector, which makes $X^T\theta$ a $m\times1$ vector which yields the answer.
	\item \[ \theta:= \theta -\frac{\alpha}{m} \sum_{i=1}^{m} [(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j]\] note that the $x$ is a column vector.
	\item the vectorized version is: \[\theta:=\theta - \frac{\alpha}{m}X^T(g(X\theta)-\bar{y})\]
	\item Advanced optimization: Given the cost function $J(\theta)$ and gradient $\frac{\partial}{\partial\theta_j}J(\theta)$ we can compute $\min_\theta J(\theta)$. 
	\item These following alorithms are more complex but compute $\theta$ at a faster rate, and there is no need to compute $\alpha$. 
	\item Conjugate gradient, BFGS, L-BGFGS
	\item Use the function "fminunc()"
	
\end{itemize}

\underline{Logistic optmization for multiple classes}
\begin{itemize}
	\item Multiple classification is the situation when $y=\{0,\ldots n\}$. i.e. we have different classes of outcomes. Our strategy is to assign one class as 'positive' and the rest of classes as 'the rest'.
	\item $y\in\{0,1,2,\ldots,n\}$
	\item $h_\theta^{(0)}(x) = P(y=0\mid x;\theta)$
		\item $h_\theta^{(1)}(x) = P(y=1\mid x;\theta)$
		\item $\vdots$
			\item $h_\theta^{(n)}(x) = P(y=n\mid x;\theta)$
	\item The prediction is $\max_i h_\theta^{(i)}(x)$
\end{itemize}

\underline{The problem of over-fitting}
\begin{itemize}
	\item Underfitting: the hypothesis function fits too poorly onto the trend of the data, the function is either too simple or accounting too little features.
	\item Overfitting: the hypothesis function is not generalized enough. It fits well with given data but presents unnecessary corners/ angles.
	\item To resolve overfitting, we can either 1) reduce the number of features, i.e. have an algorithm that ditches unimportant features. Or 2) apply regularization to reduce magnitude of some features.
\end{itemize}

\underline{Cost function} with regularization
\begin{itemize}
	\item \[\min_\theta \frac{1}{2m}\sum_{i=1}{m} (h_\theta (x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}{n}\theta_j^2\]
	\item We want a big $\lambda$ so that bumps up and forces $\theta_j$ to be small as we penalize bit $\theta_j$.
	
\end{itemize}

\underline{Gradient descent}

\begin{itemize}
	\item \begin{align*}
\text{repeat } \{ &
\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \text{ and}\\
&\theta_j := \theta_j - \alpha\big[\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j\big] \\ 
&\text{ for } j = \{1,\ldots n\}	\end{align*}
	\item or we can equivalently write this in one equation:
	\[\theta_j := \theta_j(1-\alpha\frac{\lambda}{m} - \alpha\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} ) \]
\end{itemize}

\underline{Normal Equation}
\begin{itemize}
	\item \[\theta = (X^TX+\lambda L)^{-1} X^Ty\]
	\item where $L$ is the same as $I \in M_n^{(n+1)\times (n+1) }$ 
	with the first $1$ replaced with $0$. Note that if $m<n$ then $X^TX$ is non-invertible but adding $L$ makes it invertible. Hence regulartization also solves non-invertability.	
\end{itemize}

\underline{Regularized logistic equation} \\
(note that advanced optimization method me mentioned earlier also works here)

\begin{itemize}
	\item Regularized cost function for logistic regression is:
	\[J(\theta) = -\frac{1}{m}\sum_{i=1}^{m} \big[y^{(i)}\log(h_\theta (x^{(i)} ))+ (1-y^{(i)}) \log (1-h_\theta(x^{(i)}))\big] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2\] 
	\item the last term is newly added fort he regularization
	\item Gradient descent is exactly the same	\item \begin{align*}
	\text{repeat } \{ &
	\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \text{ and}\\
	&\theta_j := \theta_j - \alpha\big[\frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j\big] \\ 
	&\text{ for } j = \{1,\ldots n\}	\end{align*}
\end{itemize}

\underline{Advanced functions} (regularization)
\begin{itemize}
	\item $Jval$ does not change while the gradients do change.
	\item Gradient $1$(index $0$):
	\[\frac{1}{m} \sum_{i=1}^{m}(h_\theta(X^{(i)} )- y^{(i)})x_0^{(i)} \]
	\item Gradient $2$(index $1,2,\ldots n$)
	\[\big(   \frac{1}{m} \sum_{i=1}^{m}(h_\theta(X^{(i)} )- y^{(i)})x_0^{(i)}   \big) +\frac{\lambda}{m}\theta_j\] note that the last term is a newly added item.
\end{itemize}
\newpage
\section*{Week 4}

\underline{Neural Networks model representation}

\begin{itemize}
	\item We use matrices and vectors to model neurons and layers. [I'm too lazy to use tikz to draw it]
	\item Neural network is consisted of multiple layers. There is an input-layer, lots of hidden layer in the middle and the output layer wich is one node. 
	\item $a_i^{(j)} = $ "activation" of unit $i$ in layer $j$.
	\item $\Theta^{(j)}$ = matrix of weights controlling function mapping from layer $j$ to layer $j+1$.
	\item \underline{Vec representation}
	 \item 
	 $$  \begin{bmatrix}
	 x_0 \\
	 x_1 \\
	 x_2 \\
	 x_3 \end{bmatrix}
	 \to
	 \left[
	 \begin{array}{ccc}
	 a_1^{(2)}\\
	 a_2^{(2)} \\
	 a_3^{(2)} 
	 \end{array}
	 \right]
	 \to h_\Theta(x)
	 $$
	 \item Layer 1 to layer 2:
	\begin{align*}
	a_1^{(2)} = & g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3 )\\	a_2^{(2)} = & g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3 )\\
	a_3^{(2)} = & g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3 )\\
	\end{align*}
	\item Layer 2 to layer 3:
	\[
	h_\Theta(x)=a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)}  + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)}+ \Theta_{13}^{(2)}a_3^{(2)}  )
	\]
	\item \underline{What is the dimension of $\Theta$?}
	\item Note that each layer has its own matrix of weights. If network has $S_j$ layers in level $j$ and $S_{j+1}$ layers in level $j+1$ then $\Theta^{(j)}$ has dimensions $S_{j+1}\times (S_j+1)$ where the extra $1$ comes from the bias node.
	\item the intuition is that NN allows nodes in its hidden layer to 'learn' its own features.
	
\end{itemize}

\underline{Vectorization of Computation}

\begin{itemize}
	\item $a_1^{(2)} = g(z_1^{(2)})$
	\item $a_2^{(2)} = g(z_2^{(2)})$
	\item $a_3^{(2)} = g(z_3^{(2)})$
	\item For layer $j$ , node $k$, $z$ is $z_k^{(j)} = \Theta_{k0}^{(j-1)}x_0 + \Theta_{k1}^{(j-1)}x_1+\ldots + \Theta_{kn}^{(j-1)}x_n$
	\item 
	$$  \begin{bmatrix}
	x_0 \\
	x_1 \\
	\vdots\\
	x_n \end{bmatrix}
	z^{(j)}
	\left[
	\begin{array}{ccc} 
	z_1^{(j)}\\
	z_2^{(j)} \\
	\vdots\\
	z_n^{(j)} 
	\end{array}
	\right]
	\to h_\Theta(x)
	$$
	\item note that \[z^{(j)} = \Theta^{(j-1)}a^{(j-1)}\] since dimentions of $\Theta$ is $S-j\times n+1$ and that dimensions of $a$ is $n+1\times 1$.
	\item We add the biad unit to layer $j$ after computing $a^{(j)}$ i.e. $a_0^{(j)} = 1$.
	\item In order to compute the fial hpothesis, we compute the final $z$ vector, where the last matrix of $\Theta$ only has $1$ row, which mulipled by a volumn will result in a real number. \[h_\Theta(x) = a^{(j+1)}=g(z^{(j+1)})\]
	\item Multi-class classification- we still use the the one-vs all method. We set $h_\Theta(x)\in\mathbb{R}^4$ if there are $4$ classes. \\
	\item \[h_\Theta(x) \approx \left[\begin{array}{ccc} 
	1\\
	0\\
	0\\
	0
	\end{array}  \right]
	or 
	\left[\begin{array}{ccc} 
		0\\
		1\\
		0\\
		0
	\end{array}  \right]
   or \left[ \begin{array}{ccc} 
   	0\\
   	0\\
   	1\\
   	0
   \end{array}  \right]
or \left[ \begin{array}{ccc} 
	0\\
	0\\
	0\\
	1
	\end{array} \right] \]  with different input $x$. It returnrs one of the standard vectors $e_i$s given a particular input.
\end{itemize}
\newpage
\section*{Week 5}
Our goal is to learn how to train NNs.

\underline{The cost function for NN}

\begin{itemize}
	\item $L$: is the total number of layers in the NN
	\item $S_l$: is the number of units not counting the bias unit in layer $l$
	\item $k$: is the number of output unit/ classes
	\item The cost function is:
	\[J(\Theta)= -\frac{1}{m}\sum_{i=1}^{m} \sum_{k=1}^{m} \big[ y_k^{(i)}\log((h_\Theta(x^{(i)}))_k) +(1-y_k^{(i)} )(\log(1-h_\Theta(x^{(i)})_k)) \big] 
	 + \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}} (\Theta_{j,i}^{(i)})^2 \]
\end{itemize}

\underline{The back propagation algorithm}
\begin{itemize}
	\item The goal is to compute $\min_\theta J(\Theta)$.
	\item look at the partial derivative of $J(\Theta)$, which is
	\[ \frac{\partial}{\partial\Theta_{i,j}^{(l)}} J(\Theta)\], The back propagation algorithm works as follows:
	\item given the training sets $\{ (x^{(1)},y^{(1)}),\ldots , (x^{(m)},y^{(m)}) \}$
	\item we set $\Delta _{i,j}^{(l)} :=0 \forall i,j$
	\item For training exmples $t=1 \sim m$:
	\begin{itemize}
		\item 1.  Set $a^{(1)}:=x^{(t)}$\\
		\item 2. Perform forward propagation to compute $a^{(l)}$ for $l=1,2,\ldots L$\\
		(i.e. set up $z$(intermediate value) and use $g(z)$ to calculate next layers, so on and so on.\\
		\item 3. $\delta^{(L)} = a^{(L)} - y^{(t)}$\\
		\item 4. compute $\delta^{(L-1)}, \delta^{(L-2)},\ldots, delta^{(2)}$ using 
		\[\delta^{(l)} = ((\Theta^{(l)}) ^T\delta^{(l+1)}) .* a^{(l)} .*(1-a^{(l)}) \]
		where the first value is the calculated value 
		and that the second value is derivative of $g$ which is $g'$.
		\[g'(z^{(l)} ) =a^{(l)} .*(1-a^{(l)})\]\\ Note: $.*$ means product form direct vector multiplication. i.e. resulting vector takes $i^{th}$ value from the product of the two $i^{th}$ values from the multiplicants.\\
		\item 5. $\Delta_{i,j}^{(l)} := \Delta_{i,j}^{(l)} + a_j^{(l)}\delta_j^{(l+1)}$\\ with vectorization we have $\Delta_{i,j}^{(l)} := \Delta_{i,j}^{(l)} + \delta_j^{(l+1)}(a_j^{(l)})^T$
		\item Hence the update is \[
		D_{i,j}^{(l)} :=\left\{
		\begin{array}{ll}
		\frac{1}{m} \Delta_{i,j}^{(l)}+\lambda_{\Theta i,j}^{(l)}&\text{ if }j\neq 0\\
		\frac{1}{m}\Delta_{i,j}^{(l)} &\text{ if } j=0
		\end{array}
		\right.
		\]
		\item In here, $D$ is the accumulator for the gradient where $\frac{\partial}{\partial\Theta_{i,j}^{(l)}} J(\Theta) = D_{i,j}^{(l)}$
		\item Very useful resources to read about:\\
		\hyperref[http://neuralnetworksanddeeplearning.com/chap2.html]{''http://neuralnetworksanddeeplearning.com/chap2.html'}
		\item Tips for implementation:\\
		1. For this part, the homework assignment did not ask you to implement these steps.\\
		2. Unrolling is basically changing a matrix into a single column vector, i.e. unrolling\\
		3. Gradient checking ensures bug-free implementation\\
		4. Initial theta is set randomly
		
	\end{itemize}
		
	\end{itemize}

\newpage
\section*{Week 6}

\underline{Evaluating a learning algorithm}
\begin{itemize}
	\item What are some ways to arrive at a better hypothesis?\\
	More examples\\
	more / less \# of features\\
	more / less values of $\lambda$
	\item To evaluate a hypothesis, we split the data into training set and the test sets. They usually have 70\% and 30\% respectively.
	\item Then we learn $\Theta$, minimize $j_{train}(\Theta)$ using the training set. Next step we evaluate how good our hypothesiss is by calculating the test set error with $J_{test}(\Theta)$.
	\item to compute the test set error for lin.reg., it is half of the square error, whereas for log.reg. it is the average of  the $err$ function where it is $1$ if hypothesis is in the right rage, $0$ otherwise.
	
\end{itemize}

\underline{Model selection}

\begin{itemize}
	\item We can break down the data-set into three data-sets:
	\[\text{training set, cross validation set, and test set} \] taking up 60\%, 20\%, 20\% respectively.
	\item We can test different degree of polynomial, evaluate their error functions.\\
	1. optimize params in $\Theta$ using training set for each degree.\\
	2. find the polynomial degree $d$ that produce least error by cross validation set.\\
	3. estimate generalized error with test set.
	
\end{itemize}

\underline{Diagnosing bias, variace and  regularization,  learning curves}

\begin{itemize}
	\item Better explanation seen in the handwritten notes- it has graph examples.

\end{itemize}

\underline{debugging a learning algorithm}

	\begin{align*}
	&\text{problem} &\text{try}\\
	&\text{high var} &\text{get more training data}\\
	&\text{high var} &\text{less features}\\
	&\text{high var}&\text{increase }\lambda\\
	&\text{high bias} &\text{get more features}\\
	&\text{high bias} &\text{add polynomial features}\\
	&\text{high bias} &\text{decrease }\lambda\\	
	\end{align*}
	Note that small NN are computationally cheap but prone to underfitting.\\
	Large NN are computationally expensive but prone to overfitting. Note that $\lambda$ regularization can be use to fix this problem.

\underline{Error analysis}
\begin{itemize}
	\item Always implement a quick implementation, and use that to decide how to spend your time.\\
	\item Plot learning curves and use that to decide about your next steps. You can manually examine your errors or impementa metric that returns your performance.
	\item For skewed classes, for example one class has very large size and another has very little, we use another error metric that accounts true and false positives and negatives. ($F$ scores)
\end{itemize}

\newpage\section*{Week 7}
\underline{SVMs}
\begin{itemize}
	\item SVM for logistic regression works with different cost funtion, that is easier to compute for the computer. It allows a large margin classifier that draws the decision boundary that stays naturally far apart from the dataset. \underline{kernels} take data as input and transform them into the required forms.
	\item When \underline{using a SVM}, we can use nice libraries suchas $liblinear$ or $libsm$. We need to choose kerner(linear kernel a.k.a. no-kernel or gaussian kernel) as well as parameter $c$.
	\item Other choices of kernel must satisfy \underline{Mercer's theorem} so that it does not diverge.
	\item for multi-classification, we can use the built-in SVM package or the one-vs-all methods.
	
	\end{itemize}

\newpage\section*{Week 8}
\underline{Unsupervised learning}
\begin{itemize}
	\item The given input has no labels
	\item the algorithm dins clustering data, and it identifies structures. It ca be used for identifying market segmentations, social network analysis, organize computer clusters, and galaxy formation/astronomical data analysis.
\end{itemize}

\underline{k-means algorithm}
\begin{itemize}
	\item This is used to identify clustering.
	\item Step 1. Initialize cluster centroids randomly
	\item Step 2. assign each point to cetroids that is the closest
	\item Step 3. move the centroids, to new means of assigned points
	\item Step 4. repeat
	\item The input is:
	\\$k$: the number of clusters.
	\\Training set: $\{x^1\ldots x^m\}$ for each $x^i\in\mathbb{R}^k$
	\\It can also be used for non-separating clusters, although it may not seem like an obvious pattern.
\end{itemize}

\underline{Optimization objectives}
\begin{itemize}
	\item $c^{(i)}$ is the index of cluster $(1\sim k)$ that $x^{(i)}$ is currently assigned to.
	\item $u_k$ is the cluster centroid $k$.
	\item $u_c^{(i)}$ is the cluster centroid that $x^{(i)}$ is currently assigned to.
	\item Our objective is: \[min_{c,\mu} J(c^{(1)}, c^{(2)},\ldots c^{(m)},\mu_1,\ldots,\mu_k) \] where\[  
	J(c^{(1)}, c^{(2)},\ldots c^{(m)},\mu_1,\ldots,\mu_k) 
	= \frac{1}{m}\sum_{i=1}^{m} \lVert x^{(i)}-\mu_c^{(i)}\rVert^2
	\]
	
\end{itemize}
\underline{Random initialize clustering centroid}
\begin{itemize}
	\item select $k<m$, randomly choose $k$ training examples, and the $\mu_1,\ldots,\mu_k$ to these examples. 
	\item To avoid bad clustering, we can initate randomly many times, and compare the $J$ and pick the one that has smallest cost to start with.
	
\end{itemize}

\underline{Pick \# of clusters}
\begin{itemize}
	\item Plot \# of clusters on x-axis and cost function on y-axis, and locate the "elbow" .
	\item Another method is to pick $k$ from what you want to do with the result of learning.
\end{itemize}

\underline{Data compression}
\begin{itemize}
	\item reducing redundant data. For example, if we're to record dataset that forms one line then using 2-d structures to record this data is redundant.
\end{itemize}

\underline{visualization}
\begin{itemize}
	\item we choose the 2 or 3 of the features and plot them. This helps us to recognize the relationships between features.
\end{itemize}

\underline{PCA: Principle component analysis}
\begin{itemize}
	\item our goal is to find a surface of a lower dimension such that is has the smallest sum of distance from each point in the dataset to the projected point.
	\item Note that PCA is not lin.reg, because PCA minimizes the perpendicular distance (has nothing to do with the outcome y) where lin.reg predicts y by minimizing vertial distance.
\end{itemize}

\underline{The PCA algorithm}

\begin{itemize}
	\item The training set is $\{x^1,\ldots x^m\}$
	\item First, we must do pre-processing (or feature scaling, mean normalization)
	\item $u_j=\frac{1}{m}\sum_{i=1}^{m}x_j^{(i)}$, and replace each $x_j^{(i)}$ with $x_i-\mu_i$
	\item We then also scale the features i.e. $x_j^{(i)} \leftarrow \frac{x_j^{(i)}-\mu_j}{s_j}$
	\item Then, we compute the vector $\mu_i$ and projections/ new representations.
	\item We reduce the data from dim $n$ to dim $k$.
	The covariance matrix is : $\Sigma :\frac{1}{m}\sum_{i=1}^{n}(x^{(i)})(x^{(i)})^T $
	\item The eigenvalues of $\Sigma$ is $ [U,S,V] = svd( \Sigma) $
	\item $U$ will be a $n\times n$ matrix whose columns are $u^1,\ldots u^m$. i.e. the eigenvectors.
\[
A = 
\left[
\begin{array}{cccc}
\mid & \mid &        & \mid \\
\mu_{1}    & \mu_{2}    & \ldots & \mu_{m}    \\
\mid & \mid &        & \mid 
\end{array}
\right] \in \mathbb{R}^{n\times m} \text{ where we choose the first k as the k dimensions}\]

so that 
\[
\mu_{reduce} = 
\left[
\begin{array}{cccc}
\mid & \mid &        & \mid \\
\mu^{1}    & \mu^{2}    & \ldots & \mu^{k}    \\
\mid & \mid &        & \mid 
\end{array}
\right]
 z^{(i)} = \mu_{reduce}^Tx^{(i)}
\]
\end{itemize}

\underline{Reconstruction from compressed representation}
\begin{itemize}
	\item $z=\mu_{reduce}^T*X$
	\item $X_{approx} = \mu_{reduce}*z$
\end{itemize}

\underline{Applying PCA}
\begin{itemize}
	\item How to choose $k$?
	\begin{align*}
	\text{The average square projection error is }&=\frac{1}{m}\sum_{i=1}^{m}\mid\mid x^{(i)}-x_{approx}^{(i)}\mid\mid ^2\\
	\text{total variance in data is }&=\frac{1}{m}\sum_{i=1}^{m}\mid\mid x^{(i)} \mid\mid ^2\\
	\end{align*}
		\item We choose $k$ to be the smallest value such that
		\[\frac{ \frac{1}{m}\sum_{i=1}^{m}\mid\mid x^{(i)}-x_{approx}^{(i)}\mid\mid ^2}{\frac{1}{m}\sum_{i=1}^{m}\mid\mid x^{(i)} \mid\mid ^2} \leq 0.01\]
		\item That is, we need $99\%$ of variance to be retained.
		\item For the algorithm implementation, $[U,S,V] = svd(\Sigma)$, where $S$ is a diagonal matrix. Pick smallest $k$ such that
		\[ \frac{\sum_{i=1}^{k}S_{ii}}{\sum_{i=1}^{m}S_{ii}} \geq0.99\]
\end{itemize}

\underline{How to use PCA to speed up learning algorithm}

\begin{itemize}
	\item When running PCA, we only run it on the training set.
	\item To speed it up, we have input $\{(x^1,y^1),\ldots (x^m,y^m) \}$ for $\forall x x\in\mathbb{R}^{10000}$. Then the extracted input is $\{z^1,z^2,\ldots z^m\in \mathbb{R}^{1000}\}$\\
	\item Now we train $\{(z^1,y^1),\ldots (z^m,y^m) \}$.
	
\end{itemize}

\underline{Summary of application of PCA}

\begin{itemize}
	\item Compression: reduce storage to store data, and speed up the learning algos.
	\item Visualization: use $k=2$ or $k=3$
	\item Note: don't use PCA to prevent overfitting, and we should use regularization instead.
	\item Note: when designing system, first try without PCA, and only use PCA is necessary.
\end{itemize}



\newpage\section*{Week 9}

\underline{Anomaly detection}
\begin{itemize}
	\item Detects anomally behaviouring data, by using Gaussian distribution
	\item Density estimation: training set $= \{x^1,\ldots x^m\} \forall x, x\in \mathbb{R}^{n}$
	\item $P(x) = \prod_{j=1}^{n} p(x_j;u_j,\theta_j^2)$
\end{itemize}

\underline{Anomaly detection algorithm}
\begin{itemize}
	\item 1. Choose features $x_i$ that you think might be anomaly
	\item 2. fit params $u_1,\ldots u_n,\theta^2_1,\ldots\theta_n^2 \to u_j=\frac{1}{m}\sum x_j^{(i)}, \theta_j^2=\frac{1}{m}\sum(x_j^{(i)} -u_j)^2$
	\item 3. given new example $x$, compute $p(x)= \prod_{j=1}^{n} p(x_j;u_j,\theta_j^2)$.
\end{itemize}

\underline{Developing and evaluating an anomaly detection system}
\begin{itemize}
	\item We use the training set, cross validation set, and the test set.
	\item To evaluate whether a training data is anomaly, we first fit the models using training and cv sets, where we pick a threshold value $\epsilon$. If $p$ returns greater than $\epsilon$, it is normal, otherwise an anomaly.
	\item We use the cv set to find $\epsilon$.
	\item Note that multivariate Gaussian distribution can detect abnormal relationships between features. It is more computationaly expensive. To do so, we must had $m>n$ or else $\Sigma$ is singular.
\end{itemize}

\underline{When should we use anomaly detection? When to use supervised learning?}
\begin{itemize}
	\item AD\\
	Very small number of positive example, and large number of negative examples\\
	there are many types of anomaly, that makes learning difficult\\
	very possible to encounter anomaly in the future that we have never seen.
	\item SL\\
	Large number of positive and negative examples\\
	future anomaly may look like previous ones\\
	there is enough positive examples for the algorithm to learn.
	
\end{itemize}

\underline{Recommender systems}
\begin{itemize}
	\item give recommendation to subscribers to a service
	\item this is a high-priority to many companies i.e. Predicting movies for N*etflix.
	\item \underline{Collaborative filtering algorithms}
	\item Let $x$ be the features for users and the $\theta$ be the features for movies.
	\item 1. Initialize $x^1, \ldots x^{nm},\theta^1,\ldots, \theta^{nu}\in \mathbb{R}^n$ to small values.
	\item 2. minimize $J(x^1, \ldots x^{nm},\theta^1,\ldots, \theta^{nu})$ using gradient descent or other advanced optimization algorithms. for example
	\begin{align*}
	x_k^i &= x_k^i - \alpha(\sum_{j:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)} -y^{i,j})\theta_k^{(i)}+\lambda x_k^{(i)})\\
	\theta_k^j &= \theta_k^j - \alpha(\sum_{i:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)} -y^{i,j})x_k^{(i)}+\lambda \theta_k^{(j)})
	\end{align*}
	\item 3. For a user with parameters $\theta$ and a movie with (learned) features $x$, predict a star rating of $\theta^Tx$.\\
	The vectorized implemenation is in video, "low rank matrix factorization".
	\item For newly registered users,we use mean normalization to predict entries with no previous data. 
	
\end{itemize}

\end{document}